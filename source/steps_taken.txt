installed ROS melodic for ubuntu 18.04
following tutorials from ROS.org, starting with:
http://wiki.ros.org/catkin/Tutorials/create_a_workspace
/home/$USER/catkin_ws

now to create a package
http://wiki.ros.org/ROS/Tutorials/CreatingPackage

(in catkin_ws/src): catkin_create_pkg <package_name> [dep1][dep2][...]

step back up to catkin_ws, run: catkin_make
then: . ~/catkin_ws/devel/setup.bash
to add the new workspace to ROS envi

edit the package.xml file to be descriptive

use catkin_make to build

found rospy tutorial for subscribing to image:
wiki.ros.org/rospy_tutorials/Tutorials/WritingImagePublisherSubscriber

added 'usb_cam' to package by doing this:
`cd ~/catkin_ws/src
git clone https://github.com/bosch-ros-pkg/usb_cam.git
cd ..
catkin_make`

then source the setup.bash file!

edited usb_cam usb_cam-test.launch
(roscd usb_cam, etc)
changed source to /dev/video1, and vis from image_view to rqt_image_view (opencv/gtk issue?)


## TO GET WORKING IN BAG SPACE ##
- roscore (start core service)
- cd ros_bags/navigate_building/
    rosbag play <bag> (play bag)
- python3 <script> <subscribe topic> <publish topic> (interact with bag)


# 10-21 - had issue converting image_raw to cv2, https://github.com/erlerobot/gym-gazebo/issues/93
- trying to put it all in a package
http://wiki.ros.org/catkin/Tutorials/create_a_workspace
http://wiki.ros.org/ROS/Tutorials/CreatingPackage

# turns out cv_boost doesnt work with python3.
- attempted to compile with python 3 instead of 2, hitting error:
    [ 66%] Building CXX object src/CMakeFiles/cv_bridge_boost.dir/module.cpp.o
   `In file included from /usr/include/boost/python/detail/prefix.hpp:13:0,
                    from /usr/include/boost/python/args.hpp:8,
                    from /usr/include/boost/python.hpp:11,
                    from /home/johnny/vision_opencv/cv_bridge/src/module.hpp:21,
                    from /home/johnny/vision_opencv/cv_bridge/src/module.cpp:35:
    /usr/include/boost/python/detail/wrap_python.hpp:50:11: fatal error: pyconfig.h: No such file or directory
    # include <pyconfig.h>
            ^~~~~~~~~~~~
    compilation terminated.
    src/CMakeFiles/cv_bridge_boost.dir/build.make:62: recipe for target 'src/CMakeFiles/cv_bridge_boost.dir/module.cpp.o' failed
    make[2]: *** [src/CMakeFiles/cv_bridge_boost.dir/module.cpp.o] Error 1
    CMakeFiles/Makefile2:930: recipe for target 'src/CMakeFiles/cv_bridge_boost.dir/all' failed
    make[1]: *** [src/CMakeFiles/cv_bridge_boost.dir/all] Error 2
    Makefile:140: recipe for target 'all' failed
    make: *** [all] Error 2`

- followed guide at https://stackoverflow.com/questions/49221565/unable-to-use-cv-bridge-with-ros-kinetic-and-python3
- changed CMAKE argument to point at python3.6 library, BUT still get error.

# going to try to convert image_raw to image/Compressed like this https://answers.ros.org/question/35183/compressed-image-to-image/
- rosrun image_transport republish raw in:=/image_raw compressed out:=/image
- it works! Now back to reading the plaques.

# for the initial choice of plaque, using tkinter to provide a GUI 
- tkinter image https://stackoverflow.com/questions/23901168/how-do-i-insert-a-jpeg-image-into-a-python-tkinter-window
- tkinter radio button https://www.python-course.eu/tkinter_radiobuttons.php
- idea is to find contours in a calibration image, use the array ( or dicitonary) of contours to provide the USER
    with a few choices, and the one that is picked is set as the ur-plaque.
    - if none of the contours are of the plaque, the threshold tipping point is changed and try again. (currently set at ~170)
    https://stackoverflow.com/questions/41504375/numpy-matrix-to-tkinter-canvas
    - https://stackoverflow.com/questions/46284901/how-do-i-resize-buttons-in-pixels-tkinter to size things correctly


# 10-23-18 going through, got it mostly working but some plaques are not showing up
- adding a debug mode to show thresheld image
- some of the hallway is too dark. added threshold customixzation to CI thresh(), not perfect though.
    - maybe better camera?
    - upon taking to advr, going to run sobel or laplacian before thresholding and getting contours.
    https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_gradients/py_gradients.html


# 10-25-18 trying to grab lost plaques
- monkeying with setting on contour kernel, opening and closing, etc
- double-processing each image (or tripel) with differnt threshold values
    - actually grabs a few more, still missing 254


# 10-31-18 working on building neural net for plaque-finding,
- based on excellent https://www.pyimagesearch.com/2017/12/04/how-to-create-a-deep-learning-dataset-using-google-images/
- grabbed images of ada signs, images of nulletin boards, and images of hallways
- plan to try with just images as good set and then halls and bulletin boards os bad set 
    - better would be to grab plaques from images in good test set and paste them into the hallway images
    - could use this with the images i generated earlier
- after download will train and evaluate as I work on map creation.
! Using this link: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html
    - will combine this with my image generation
    - grabbing images with staris, exitsd, numbers, office signs to increase those found
    - also going through other datasets to remove bad ones (that have plaques) and dropping them into the good plaque folder
# use image hash to prevent collisions/ wed out identical images?
- https://www.pyimagesearch.com/2017/11/27/image-hashing-opencv-python/
    - in interest of time will probably do something more simple

* kinda neat, python builtin https://www.tutorialspoint.com/python/string_endswith.htm
* where i got the reflective border info: https://docs.opencv.org/3.1.0/d3/df2/tutorial_py_basic_ops.html

# 11-1-18 training day
- caut-and-paste of keras model
- stupid question: once model is trained, how do i USE IT?
    - https://stackoverflow.com/questions/35074549/how-to-load-a-model-from-an-hdf5-file-in-keras
    - https://keras.io/applications/
    * from keras.models import load_model
    * model = load_model('model.h5')
    https://mail.google.com/mail/u/0/#inbox|https://www.google.com/search?num=50&client=firefox-b-1-ab&ei=1cTZW8mbIpCr8APq_aPoAQ&q=character+recognition+using+keras+and+python3&oq=character+recognition+using+keras+and+python3&gs_l=psy-ab.3...27673.28163..28428...0.0..0.182.607.2j3......0....1..gws-wiz.......0i71.zF9PljcI8Jw|http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/|https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html|https://keras.io/models/about-keras-models/|https://curl.haxx.se/|https://gist.githubusercontent.com/fchollet/0830affa1f7f19fd47b06d4cf89ed44d/raw/47d3e33764c902ed33a64f35f5f68d911de05d8d/classifier_from_little_data_script_1.py|https://stackoverflow.com/questions/35074549/how-to-load-a-model-from-an-hdf5-file-in-keras|https://keras.io/applications/|https://keras.io/preprocessing/image/#image-preprocessing|https://www.thewindowsclub.com/copy-urls-open-tabs-firefox-chrome

- back to text reading
    - have pytesseract installed, but not tesseract ocr
        - sudo apt-get install tesseract-ocr


# 11-4-18 try number two
- the results of running my trained network were ... not useful.
- going to use https://www.pyimagesearch.com/2017/12/11/image-classification-with-keras-and-deep-learning/
    - in conjucntion with the keras tutorial to hopefully learn how it works.
    - following this toturial and running against my dataset from google
- error running model to train: FileNotFoundError: [WinError 3] The system cannot find the path specified: 'data/train'
    - wrong diorecotry :()
- got it working with loading model, seemed like there was some correlation with the images to plaques (~0.1-0.2 for most plaque images)
    - must try again with my own generated dataset
    - TODO: better script writing in training set
- back to survey ###
    - using paramiko to ssh survey file, then sqlite3 to rad it into lists and make it nice https://stackoverflow.com/questions/29508743/download-files-over-ssh-using-python

# 11-5-18 evaluating dataset
- using MakeTrainingData.py and the hallway generator to create data. will use either method and evaluate which is better.
- today is the random images from makeTrainingData.py
    - generating 3000 each positive and negative images.
    - creating them at 150X150
    - ARGGH! images are repeating after only 10-20 iterations. what the hell?
    - https://stackoverflow.com/questions/2145510/random-is-barely-random-at-all
    - https://en.wikipedia.org/wiki/Mersenne_Twister
    - thanks to the 'duh' moment from the SO question,
        i made a sequential list of numbers, randomized that, and used that number
        as the random seed for the image generator.
        - after this, still some repeaters! Damnit!
            ~ 348_pos and 387_pos - similar, but different background lines.
            ~ 1495, 1533 same same!
        - changed the passing of the random seed from image generator to image object. each image will have the same seed as its parent.
        - still have some that arte teh exact same EXCEPT for the salt and pepper, which is through numpy.random. this leads me to beleive that
            once an image gets a specific seed, it will make the same image. Changed implementation of random rectangles and lines to hoepfuilly 
            switch it up.
            - looks like this was effective enough to pass the QuickLookâ„¢ visaulk inspection.
            - will now run through the Keras tutorial CNN

# 11-6-18 I VOTED (but it didn't make a difference)
- trouble with two huge ( > 390mb) files in a staged commit.
- found thi guide to git https://sethrobertson.github.io/GitFixUm/fixup.html#remove_last
    - git reset HEAD~2 (reset last two commits without removing the work done on them, no HARD command)
    - seems to have worked
    
# 11-7-18 The git solution DID NOT work
- went here https://docs.microsoft.com/en-us/azure/devops/articles/remove-binaries?view=vsts at suggestion of Dr Schw
    - found SHA of last good commit (git log)
    - git rebase -i <SHA>
    - didn't work: https://stackoverflow.com/questions/33911379/git-rebase-fatal-needed-a-single-revision-invalid-upstream-i/33911409
        - git branch --set-upstream-to=origin/master
    - wnet through and merged changes
    - git push --force put me back where i belong

# 11-8-18 Checking out results from fabricated data set and working on map
- argh, they suck!
- i am training lots of data on a net designed for limited data ... this may be part of problem?
- even running on the validation data, some are 1-0.7, but many or .01, etc
    - i think the fabricated random data is not the answer.
    - will try to build model with evaluation doen on b/w images
    - will return to the hallway idea, BUT:
        - change up the background color to fall in a range of values
        - no need for color information (to simplify lighting changes?)
        - add random light and dark rectangles to simulate posters and stuff
        - add black and white areas in door areas to simulate open doors with bright windows

- RUNNING FROM A DIFFERENT TUTORIAL (https://medium.com/@kylepob61392/airplane-image-classification-using-a-keras-cnn-22be506fdb53)
    - my tensors brought all the GPUs to the yard 
            2018-11-08 13:58:36.316351: W T:\src\github\tensorflow\tensorflow\core\framework\op_kernel.cc:1275] OP_REQUIRES failed at conv_ops.cc:398 : Resource exhausted: OOM when allocating tensor with shape[200,92,142,142] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
            Traceback (most recent call last):
            File "source\SmartImage\other_classifier.py", line 184, in <module>
                model.fit(x_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks, verbose=0)
            File "C:\Users\TJAMS002\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\engine\training.py", line 1037, in fit
                validation_steps=validation_steps)
            File "C:\Users\TJAMS002\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\engine\training_arrays.py", line 199, in fit_loop
                outs = f(ins_batch)
            File "C:\Users\TJAMS002\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\backend\tensorflow_backend.py", line 2666, in __call__
                return self._call(inputs)
            File "C:\Users\TJAMS002\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\backend\tensorflow_backend.py", line 2636, in _call
                fetched = self._callable_fn(*array_vals)
            File "C:\Users\TJAMS002\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\client\session.py", line 1382, in __call__
                run_metadata_ptr)
            File "C:\Users\TJAMS002\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\errors_impl.py", line 519, in __exit__
                c_api.TF_GetCode(self.status.status))
            tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[200,92,142,142] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
                    [[Node: conv2d_4/convolution = Conv2D[T=DT_FLOAT, _class=["loc:@training/Adam/gradients/conv2d_4/convolution_grad/Conv2DBackpropInput"], data_format="NCHW", dilations=[1, 1, 1, 1], padding="VALID", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/device:GPU:0"](activation_3/Relu, conv2d_4/kernel/read)]]
            Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

                    [[Node: metrics/acc/Mean_1/_141 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_908_metrics/acc/Mean_1", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]
            Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
    - will make images smaller? I feel like the plaque will dissappear into a single pixel and be meaningless
        - this may be why the previous results were ambiguous
        - may be fixed with the synthetic hallway
        - changed BATCH_SIZE from 200 to 32 (line 168, otherclassifier.py)
            - it runs now, but results are ~ 50% (baaaaaaaaad)
        - replaced the CNN with the one from the keras tutorial, got 100% accuracy (must have done it wrong)
            - still nonsense when running it on the training data.
!!! It is becoming obvious that my lack of understanding is getting in the way of my using this technology properly.
    - i will try the original version of this tut, as well as the keras one, with the other generated data.
>>>>>>>>>> https://www.pyimagesearch.com/2014/11/10/histogram-oriented-gradients-object-detection/ <<<<<<<<<<<<<<<<<
>>>>>>>>>> this way may be better for the hallway datasets. <<<<<<<<<<
>>>>>>>>>> https://www.learnopencv.com/histogram-of-oriented-gradients/ <<<<<<<<<
>>>>>>>>>> https://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf <<<<<<<<<

# 11-13-18 Finally have idea for theme of thesis
- problems exist in the world, and many of them have solutions that can be constructed from existing technology
- three stages:
    1: try the simplest programatic way to solve the problems
        - user interaction to calibrate plaque size, find rectangles, make simple map
        - out-of-the-box OCR for python
        - this is mostly done (need to test with better data)
    2: use machine learning to find images with plaques and to decode the words
        - this stage is not done yet
        - use SVM and HOG to train on found data, generated data, and recreated data
        - SVM and HOG on chars74k dataset to learn letters and numbers
    3: use Neural Nets and other deep-learning techniques
        - keras CNNs on the three data types for plaque finding
        - same for character recognition
    >>>>>> FUTURE MOVES <<<<<<
        - instead of simple video, use SLAM or some other way to get relational metric information
            about the features and plaques to create a better map
- three types of data:
    1: found data
        - from google image search (not huge)
    2: generated data
        - plaques or not plaques with random shit in the backgrounds
    3: recreated data:
        - create image of hallway, with posters, papers, doorways, etc
        - sample parts of these images to generate more realistic dataset
    >>>>>> FUTURE MOVES <<<<<<      
        - use these plaque finding techniques to create a better dataset from real images
            and then train better networks on this data

# with that in mind, working on the hallway creator
- add sensibly-placed stuff in background (papers and posters)
- skew, stretch, rotate, scale, and flip snapshot images

~~ I find that as I have been learning more of the python style guides and go back to working on previous code,
        I run into problems such as:
        in hall_driver, i have this line:
            temp.save(imagePath=''.join([directory,str(TL[0]),'_',str(TL[1]),'.',str(BR[0]),'_',str(BR[1]),'.',str(n),'.png']))
        but I have changed CustomImage.Image.save() to take file_path instead (more descriptive, and snake case)
        so i get this error:
            Traceback (most recent call last):
            File "source\ImageGeneration\hall_driver.py", line 50, in <module>
                main(args.directory, args.images)
            File "source\ImageGeneration\hall_driver.py", line 32, in main
                str(n),'.png']))
            TypeError: save() got an unexpected keyword argument 'imagePath'
        causing me to need to go back and figfure out what i changed.
    ~~~ If i had unit tests to run each time before I commit a change, this would not happen as often!
        - need to do unit testing at least a little bit

# 11-13-18 running in the hallway
- initial test put too much crap right in front of the door!
    - instead of making a clear area, i'll specify a no-go area
    - any points that are randomly gen'd that lie in this area will be re-rolled
    - best way to do this?
        - instead of an if/loop thing, maybe create a list of possible x or y values,
            and randomly grab one from there
- wrestling a little with it, when i think:
    ?? why not just place plaque and door over the background stuff ??

# 11-15-18 Time to start writing this beast :^(
- https://www.evl.uic.edu/spiff/fear/thesis/index.html
- https://writerspk.com/computer-science-thesis/
-= http://www.cs.toronto.edu/~sme/presentations/thesiswriting.pdf

- also, thought on keras and ml models:
    >>> instead of feeding the image itself to the classifier, maybe send information ABOUT the image,
        such as contours, contour areas, etc
        - since the thing we're looking for (a colored rectangle) has no inherent difference from
            anything else in the image
        - if i can find text, location of the text box would be good too
        >>> maybe swipe over images and try to read text in each spot?

- multiple images at once: https://stackoverflow.com/questions/46615554/how-to-display-multiple-images-in-one-figure-correctly
    - basically just copying it fopr now, since it seems to work well

    In [30]: arr = [[1,23],[23,200],[450,11],[512,512]]

In [31]: type(arr)
Out[31]: list

In [32]: bar = np.float32(arr)

In [33]: bar
Out[33]:
array([[  1.,  23.],
       [ 23., 200.],
       [450.,  11.],
       [512., 512.]], dtype=float32)

In [34]: isinstance(arr, list)
Out[34]: True

In [35]: isinstance(bar, list)
Out[35]: False

In [36]: isinstance(bar, numpy.ndarray)
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-36-71ec08aa9573> in <module>()
----> 1 isinstance(bar, numpy.ndarray)

NameError: name 'numpy' is not defined

In [37]: isinstance(bar, np.ndarray)
Out[37]: True

## 11-27-18 writing time
- finding as i write that the final result of the project is going to be more limited than I had anticipated
- BUMMER
- Should have started writing a few months ago, but here we are
- Need to finalize a graph-making system before I can really write much more
    - must make assumptions about the results of the palque-reading system
Monday: setup
https://circuitdigest.com/microcontroller-projects/license-plate-recognition-using-raspberry-pi-and-opencv
https://circuitdigest.com/microcontroller-projects/optical-character-recognition-ocr-using-tesseract-on-raspberry-pi
* set up environment
* image processing and normalization pipeline
[open] -> [size] -> [gray] -> [blur] -> [edges] -> [contours] -> [crop] -> [OCR]
* can use the human-interaction which dictates whaich size of rectangle to grab fopr a session

Tuesday:
https://scikit-image.org/docs/stable/api/skimage.exposure.html -> smarter grays
* rework markPlaque in ShapeDetection to take and return things properly
* finish man-interaction pipeline
* use feature-free ur-image to provide 'human input' ( proper camera location, with obvious plaque in center for calibration)
* HOG and other image dectector 
    * https://stackoverflow.com/a/37005437
    * https://docs.opencv.org/2.4/modules/gpu/doc/object_detection.html
    * https://www.pyimagesearch.com/2015/11/16/hog-detectmultiscale-parameters-explained/

Wednesday: 
* Running pipeline with simpel ROI detection (bounding boxes) and east (from pyimagesearch).
    east seems to do a better job of grabbing the interest areas. Both could benefit from only grabbing the largest area in the image.
* hump day.
    getting lots on noise from current dataset, given that they are variant in shooting angle

Thursday:
* train HOG to find plaques
* ???

Friday:
* ???